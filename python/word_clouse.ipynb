{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd87ea3-0078-4981-ba93-6367a37bc436",
   "metadata": {},
   "source": [
    "### Extracting cloud word data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf6afe-b7b6-43e8-b1f4-27b6e5452dde",
   "metadata": {},
   "source": [
    "A lot more can be done using the text book, we will now explore how to extract the word describing the most each book with the goal of doing a WordCloud like visualisation. To do so, we will need to reload the different file and convert each book into a very large string containing all the text of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f66ffb-8a4b-4db3-9ed3-0771f9618ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from helper.constantes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83d74f5-54e3-4985-a60b-295053b4fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_books = []\n",
    "for i in range(1,8):\n",
    "    # Read the full book line by line in the a list\n",
    "    with open(data_folder+hpbooks_folder+f\"hp{i}.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    cur_str = \"\"\n",
    "    for line in lines:\n",
    "        cur_str += line\n",
    "    all_books.append(cur_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994a0091-a5e5-4196-8331-970a220f455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fit_transform_return(corpus, min_df=0.2, max_df=1.0, ngram_range =(1,4)):\n",
    "    final_stopwords_list = stopwords.words('english')+ [\"said\", \"page\", \"mind\"]\n",
    "    tfidf = TfidfVectorizer(min_df=min_df,max_df=max_df,stop_words=final_stopwords_list, use_idf=True,ngram_range=ngram_range)\n",
    "    X = tfidf.fit_transform(corpus)\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())\n",
    "    return X, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95bf7a7-d2ea-4db0-bde8-0c23c60d93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_top_tf_idf_words(full_x, feature_names, top_n=2):\n",
    "    def get_top_tf_idf_words(x):\n",
    "        sorted_nzs = np.argsort(x.data)[:-(top_n+1):-1]\n",
    "        return feature_names[x.indices[sorted_nzs]]\n",
    "    return [get_top_tf_idf_words(cur) for cur in full_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374bdd23-9294-4dd6-9b7e-1948ba99daac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vf/btrykr5n65v_k_lwb61317k80000gn/T/ipykernel_21922/1901225924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_transform_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_books\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_all_top_tf_idf_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vf/btrykr5n65v_k_lwb61317k80000gn/T/ipykernel_21922/4109288524.py\u001b[0m in \u001b[0;36mfit_transform_return\u001b[0;34m(corpus, min_df, max_df, ngram_range)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_stopwords_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x, feat = fit_transform_return(all_books, 0.2, 1.0, (1,4))\n",
    "get_all_top_tf_idf_words(x,feat, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd840d-3473-4ac8-8af6-9561d5d7f20d",
   "metadata": {},
   "source": [
    "We can see that we specify a lot of different parameters to extract the most important word in each book. Furthermore, we can see that the most of the time, the words \"Harry\", \"Potter\", \"Hermione\" or \"Ron\" come in the results. This is due to the fact taht these characters are so central in the saga. However, if we want to have an interseting visualisation, we will have to get different words for each book. To do so, we can tune the parameters by first requiring that the document frequency is below 1.0, i.e. the words doesn't appear in all the books. Furthermore, as already shown, we remove the most common english stopwords. Also, we added custom stopword such as said or minds that describe the process of talking in the book. Below, we consider a second version using a maximal document frequeny of 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e84ec-6ec3-4bea-b794-ec7bc44fff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, feat = fit_transform_return(all_books, 0.2, 0.9, (1,4))\n",
    "get_all_top_tf_idf_words(x,feat, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9131c2-a021-471b-9631-ce30b6c5529e",
   "metadata": {},
   "source": [
    "Just by removing the words or n-grams appearing in every book, we fall on meaningful words for each book. Indeed, we can see that all the words that appears for each book correspond to specific events that happen only in that book or not too often. Using the results we got, we will be able to produce the expected CloudWord visualisations and make it evolve over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab63f05-56f1-43a2-a3fe-4342fa116cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
